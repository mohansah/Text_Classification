{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MOHAN KUMAR SAH\\.conda\\envs\\python3.5\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import LSTM, Dense, Embedding, Conv1D, MaxPooling1D, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>CRIME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 5...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       category\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...          CRIME\n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...  ENTERTAINMENT\n",
       "2  Hugh Grant Marries For The First Time At Age 5...  ENTERTAINMENT\n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...  ENTERTAINMENT\n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...  ENTERTAINMENT"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data into Pandas dataframe\n",
    "df = pd.read_csv('news_data.csv',encoding='latin-1')\n",
    "\n",
    "# Printing first 5 rows\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200853, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing dimension of dataset (Rows, Columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200853 entries, 0 to 200852\n",
      "Data columns (total 2 columns):\n",
      "text        200853 non-null object\n",
      "category    200853 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Information about Null Values and data types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>FilterText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>mass shootings texas last week tv left husband...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>smith joins diplo nicky jam world cup official...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 5...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>hugh grant marries first time age actor longti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>jim carrey blasts castrato adam schiff democra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>julianna margulies uses donald trump poop bags...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       category  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...          CRIME   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...  ENTERTAINMENT   \n",
       "2  Hugh Grant Marries For The First Time At Age 5...  ENTERTAINMENT   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...  ENTERTAINMENT   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...  ENTERTAINMENT   \n",
       "\n",
       "                                          FilterText  \n",
       "0  mass shootings texas last week tv left husband...  \n",
       "1  smith joins diplo nicky jam world cup official...  \n",
       "2  hugh grant marries first time age actor longti...  \n",
       "3  jim carrey blasts castrato adam schiff democra...  \n",
       "4  julianna margulies uses donald trump poop bags...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering out unncessary information from given text\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import Word\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Removing punctuations and all digits from text\n",
    "filterString = string.punctuation + '“”|”' + string.digits\n",
    "df['FilterText'] = df['text'].apply(lambda x: x.translate(str.maketrans(filterString,' '*len(filterString),'')))\n",
    "\n",
    "# Removing all single characters\n",
    "df['FilterText'] = df['FilterText'].replace('\\s+[a-zA-Z]\\s+', ' ', regex=True)\n",
    "\n",
    "# Removing single characters in beginning\n",
    "df['FilterText'] = df['FilterText'].replace('\\^[a-zA-Z]\\s+', ' ', regex=True)\n",
    "\n",
    "# Removing multiple spaces\n",
    "df['FilterText'] = df['FilterText'].replace('\\s+', ' ', regex=True)\n",
    "\n",
    "# Converting text to lowercase\n",
    "df['FilterText'] = df['FilterText'].apply(lambda x: x.lower())\n",
    "\n",
    "# Removing stop words from text\n",
    "df['FilterText'] = df['FilterText'].str.split(' ').apply(lambda x: ' '.join(k for k in x if k not in stop))\n",
    "\n",
    "# Lemmatizing all words in the text\n",
    "df['FilterText'] = df['FilterText'].apply(lambda x: \"\".join([Word(word).lemmatize() for word in x]))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories\n",
      "-------------------------------------------------------------------------------------\n",
      "['CRIME' 'ENTERTAINMENT' 'WORLD NEWS' 'IMPACT' 'POLITICS' 'WEIRD NEWS'\n",
      " 'BLACK VOICES' 'WOMEN' 'COMEDY' 'QUEER VOICES' 'SPORTS' 'BUSINESS'\n",
      " 'TRAVEL' 'MEDIA' 'TECH' 'RELIGION' 'SCIENCE' 'LATINO VOICES' 'EDUCATION'\n",
      " 'COLLEGE' 'PARENTS' 'ARTS & CULTURE' 'STYLE' 'GREEN' 'TASTE'\n",
      " 'HEALTHY LIVING' 'WORLDPOST' 'GOOD NEWS' 'FIFTY' 'ARTS' 'WELLNESS'\n",
      " 'PARENTING' 'HOME & LIVING' 'STYLE & BEAUTY' 'DIVORCE' 'WEDDINGS'\n",
      " 'FOOD & DRINK' 'MONEY' 'ENVIRONMENT' 'CULTURE & ARTS']\n"
     ]
    }
   ],
   "source": [
    "# printing all the unique categories in the dataset\n",
    "print(\"Categories\\n-------------------------------------------------------------------------------------\")\n",
    "print(df['category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different Category =  40\n"
     ]
    }
   ],
   "source": [
    "# printing number of unique categories in the dataset\n",
    "print(\"Number of different Category = \",len(df['category'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature  (200853,)\n",
      "Shape of target  (200853,)\n"
     ]
    }
   ],
   "source": [
    "# Separating the FilterText and category columns\n",
    "\n",
    "feature=df.FilterText\n",
    "target=df.category\n",
    "\n",
    "print(\"Shape of feature \",feature.shape)\n",
    "print(\"Shape of target \",target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6],\n",
       "       [10],\n",
       "       [10],\n",
       "       ...,\n",
       "       [28],\n",
       "       [28],\n",
       "       [28]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process the labels i.e., assigning the numerical value to words\n",
    "\n",
    "# create the Label encoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# convert the categorical columns into numeric by assigning a numerical lebel to categorical label\n",
    "target = le.fit_transform(target)\n",
    "\n",
    "# Reshape the target\n",
    "target = target.reshape(-1,1)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encode target column\n",
    "target = to_categorical(target)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test data\n",
    "\n",
    "feature_train, feature_test, target_train, target_test = train_test_split(feature,target,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data\n",
    "# Tokenize the data and convert the text to sequences.\n",
    "\n",
    "max_words = 1000\n",
    "\n",
    "# Create Tokenizer\n",
    "tok = Tokenizer(num_words = max_words) # num_words: the maximum number of words to keep, based on word frequency.\n",
    "\n",
    "# Train the Tokenizer to the texts\n",
    "tok.fit_on_texts(feature_train)\n",
    "\n",
    "# Convert list of strings into list of lists of integers\n",
    "train_sequences = tok.texts_to_sequences(feature_train)\n",
    "test_sequences = tok.texts_to_sequences(feature_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to feed this data into our CNN, all input documents must have the same length. We will limit the maximum review length to max_len by truncating longer reviews and padding shorter reviews with a null value (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Padding to ensure that all the sequences have the same shape.\n",
    "\n",
    "max_len = 150\n",
    "\n",
    "train_sequences_matrix = sequence.pad_sequences(train_sequences, maxlen=max_len)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "\n",
    "embedding_size = 50    # length vectors to represent each word\n",
    "model.add(Embedding(max_words, embedding_size, input_length=max_len))   # Embedded layer that uses 50 length vectors to represent each word\n",
    "model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))  # Convolutional Layer\n",
    "model.add(MaxPooling1D(pool_size=2))   # Pooling Layer, We are using Max pooling\n",
    "model.add(LSTM(100))   # LSTM Layer with 100 memory units (smart neurons)\n",
    "model.add(Dropout(0.5))      # Dropout Layer, to reduce overfitting in the LSTM models\n",
    "model.add(Dense(40, activation='softmax'))  # Dense layer, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 150, 50)           50000     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 150, 64)           9664      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 75, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 40)                4040      \n",
      "=================================================================\n",
      "Total params: 129,704\n",
      "Trainable params: 129,704\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Printing the summery of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary is textual and includes information about:\n",
    "1. The layers and their order in the model.\n",
    "2. The output shape of each layer.\n",
    "3. The number of parameters (weights) in each layer.\n",
    "4. The total number of parameters (weights) in the model.\n",
    "\n",
    "Next, we need to compile our model. Compiling the model takes three parameters: optimizer, loss and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model using accuracy as a measure of model performance\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The optimizer controls the learning rate. We will be using ‘adam’ as our optmizer. Adam is generally a good optimizer to use for many cases. The adam optimizer adjusts the learning rate throughout training.\n",
    "2. The learning rate determines how fast the optimal weights for the model are calculated. A smaller learning rate may lead to more accurate weights (up to a certain point), but the time it takes to compute the weights will be longer.\n",
    "3. ‘binary_crossentropy’ is used for our loss function. This is the most common choice for classification. A lower score indicates that the model is performing better.\n",
    "4. ‘accuracy’ metric is used to see the accuracy score on the validation set when we train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160682 samples, validate on 40171 samples\n",
      "Epoch 1/5\n",
      "160682/160682 [==============================] - 754s 5ms/step - loss: 0.0860 - acc: 0.9776 - val_loss: 0.0746 - val_acc: 0.9793\n",
      "Epoch 2/5\n",
      "160682/160682 [==============================] - 445s 3ms/step - loss: 0.0741 - acc: 0.9793 - val_loss: 0.0710 - val_acc: 0.9797\n",
      "Epoch 3/5\n",
      "160682/160682 [==============================] - 505s 3ms/step - loss: 0.0713 - acc: 0.9796 - val_loss: 0.0694 - val_acc: 0.9799\n",
      "Epoch 4/5\n",
      "160682/160682 [==============================] - 443s 3ms/step - loss: 0.0698 - acc: 0.9799 - val_loss: 0.0683 - val_acc: 0.9801\n",
      "Epoch 5/5\n",
      "160682/160682 [==============================] - 449s 3ms/step - loss: 0.0687 - acc: 0.9801 - val_loss: 0.0678 - val_acc: 0.9801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12c32fc36d8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "model.fit(train_sequences_matrix, target_train, batch_size=128, validation_data=(test_sequences_matrix,target_test), epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train, we will use the ‘fit()’ function on our model with the following parameters: training data (train_sequences_matrix), target data (target_train), validation data, and the number of epochs.\n",
    "\n",
    "1. train_sequences_matrix: Features with which we train our model\n",
    "2. target_train: Target with which we train our model corresponding to featues\n",
    "3. validation_data: For our validation data, we will use the test set provided to us in our dataset, which we have split into test_sequences_matrix and target_test.\n",
    "4. epochs: one epoch stands for one complete training of the neural network with all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0090187 , 0.00222649, 0.01012357, ..., 0.01582009, 0.00259226,\n",
       "        0.01764576],\n",
       "       [0.01892316, 0.00632548, 0.02367266, ..., 0.00254493, 0.00716328,\n",
       "        0.0243885 ],\n",
       "       [0.00349446, 0.00127455, 0.00251856, ..., 0.00441868, 0.00015059,\n",
       "        0.00056355],\n",
       "       ...,\n",
       "       [0.00049652, 0.00026057, 0.00091808, ..., 0.00456096, 0.00017685,\n",
       "        0.00069567],\n",
       "       [0.00307324, 0.00439842, 0.00544752, ..., 0.03995791, 0.00105476,\n",
       "        0.00179401],\n",
       "       [0.00093183, 0.00271806, 0.04894731, ..., 0.00182778, 0.01132879,\n",
       "        0.01357007]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making predictions\n",
    "preds=model.predict(test_sequences_matrix)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original values in target_test\n",
    "target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40171/40171 [==============================] - 41s 1ms/step\n",
      "Test set:  Accuracy: 98.006 % and Loss: 6.785 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "accr = model.evaluate(test_sequences_matrix,target_test)\n",
    "print('Test set:  Accuracy: {:0.3f} % and Loss: {:0.3f} %'.format(accr[1]*100,accr[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally we get 98.006 % accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
